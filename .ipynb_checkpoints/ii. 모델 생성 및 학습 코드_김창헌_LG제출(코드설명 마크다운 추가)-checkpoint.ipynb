{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이 코드는 데이콘 통계왕 님께서 공유한 코드를 기반으로 수정된 코드이며 코드 설명에는 아래와 같은 사이트를 참고했음을 밝힙니다.\n",
    "\n",
    "참고 코드 : \n",
    "\n",
    "https://dacon.io/competitions/official/235640/codeshare/1703?page=1&dtype=recent&ptype=pub\n",
    "    \n",
    "\n",
    "설명 참고사이트 :\n",
    "\n",
    "https://wikidocs.net/book/2155\n",
    "\n",
    "https://blog.pingpong.us/tpu-with-tf2-and-gcp/\n",
    "\n",
    "https://www.kaggle.com/docs/tpu\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data\n",
    "\n",
    "https://hcnoh.github.io/2018-11-05-tensorflow-data-module\n",
    "\n",
    "https://www.tensorflow.org/guide/data_performance?hl=ko\n",
    "\n",
    "https://www.tensorflow.org/guide/autodiff?hl=ko\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape#used-in-the-notebooks_1\n",
    "\n",
    "https://teddylee777.github.io/tensorflow/gradient-tape\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이 코드는 아래와 같은 환경에서 수행되었습니다. \n",
    "\n",
    "VM : Colab Pro(TPU)\n",
    "\n",
    "tensorflow :  2.2\n",
    "\n",
    "rdkit :  2020.03.3\n",
    "\n",
    "accelerator :  colab TPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwkGGPg3RIrI"
   },
   "source": [
    "### 구글 드라이브 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 18396,
     "status": "ok",
     "timestamp": 1602460190819,
     "user": {
      "displayName": "David Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYaI7z-5kRwPA6R2AefJobuVfXBJtTIaLtVS1bHw=s64",
      "userId": "16225775759515109894"
     },
     "user_tz": -540
    },
    "id": "E9ja6eV7uUyn",
    "outputId": "e01faa70-636f-48e5-f7b0-fd650b6f22ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXp82JAURLou"
   },
   "source": [
    "### 텐서플로우 2.2설치 (2.3은 불안정합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 109586,
     "status": "ok",
     "timestamp": 1602336159158,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "KAXIze1yEnZR",
    "outputId": "229abaee-f72b-41dc-9fa5-760cc84baeba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow~=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/e3/663eac537202dee730ad6e61769fc3ebce92a6085dbfd13ca902df5f1477/tensorflow-2.2.1-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 26kB/s \n",
      "\u001b[?25hCollecting tensorflow_gcs_config~=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/0b/c6fe9b75ba3999c10b7ab77e7d4019c90344bb79609a9378355ca6712598/tensorflow_gcs_config-2.2.0-py3-none-any.whl (392kB)\n",
      "\u001b[K     |████████████████████████████████| 399kB 59.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.35.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.12.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.12.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.32.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 51.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.3.3)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 55.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow~=2.2.0) (50.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.17.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.8)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, tensorflow-gcs-config\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "  Found existing installation: tensorflow-gcs-config 2.3.0\n",
      "    Uninstalling tensorflow-gcs-config-2.3.0:\n",
      "      Successfully uninstalled tensorflow-gcs-config-2.3.0\n",
      "Successfully installed tensorboard-2.2.2 tensorflow-2.2.1 tensorflow-estimator-2.2.0 tensorflow-gcs-config-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import os\n",
    "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
    "if resp.status_code != 200:\n",
    "  print(\"Failed to switch the TPU to TF {}\".format(version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niBKPsu8ROiB"
   },
   "source": [
    "### Rdkit 설치합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 112220,
     "status": "ok",
     "timestamp": 1602336163499,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "0EtT_h2g30nb",
    "outputId": "1447d6bd-0161-42b1-f86e-e3bb14bab621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |███████▉                        | 10kB 23.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 20kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 30kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 40kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# rdkit 2020.03.3 버전 다운로드\n",
    "!pip install kora -q\n",
    "import kora.install.rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGXgxmpERXRg"
   },
   "source": [
    "### rdkit, tensorflow, efficientNet, numpy, pandas등 필요한 여러 모듈을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 120211,
     "status": "ok",
     "timestamp": 1602336173704,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "6lgZW3Awboug",
    "outputId": "859644f0-be63-4baf-d703-30be94db7d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/85/75/3d669945d41e5aedd5c4333b9dc6192b7839d2bafd04b75b8222d4e92ae0/gcsfs-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.17.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n",
      "Collecting aiohttp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 5.1MB/s \n",
      "\u001b[?25hCollecting fsspec>=0.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/38/39b83c70ff47192255c15da1b602322cb9918682199d5c1d9cf128bdd531/fsspec-0.8.3-py3-none-any.whl (88kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.1.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (50.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c9/379b807a9c298b9694d0af8ee4260be7d40ab1a11fb9d4ae9e70b1e69d96/yarl-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (257kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 17.5MB/s \n",
      "\u001b[?25hCollecting multidict<5.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 22.9MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.0.4)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (20.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
      "Building wheels for collected packages: idna-ssl\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=1fa2c7784c20dc0974034b0a4d171a62b566bac90491069ab5c6701032d33380\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
      "Successfully built idna-ssl\n",
      "Installing collected packages: multidict, yarl, async-timeout, idna-ssl, aiohttp, fsspec, gcsfs\n",
      "Successfully installed aiohttp-3.6.2 async-timeout-3.0.1 fsspec-0.8.3 gcsfs-0.7.1 idna-ssl-1.1.0 multidict-4.7.6 yarl-1.6.0\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import RDConfig\n",
    "from rdkit import RDLogger\n",
    "from rdkit import rdBase\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "\n",
    "\n",
    "\n",
    "!pip install -q efficientnet >> /dev/null\n",
    "import efficientnet.tfkeras as efn\n",
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf, re, math\n",
    "import tensorflow.keras.backend as K\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import random, re, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "!pip install gcsfs #gcp 파일 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS3Qm65MRald"
   },
   "source": [
    "## TPU 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab에서 TPU를 사용할 수 있습니다. TPU를 사용하기 위해서 다음과 같은 절차를 따릅니다.\n",
    "- TPUClusterResolver를 통해서 하나 혹은 여러 대의 TPU집합을 잡습니다. \n",
    "    - tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "\n",
    "- experimental_connect_to_cluster 함수를 통해 TPU와 런타임을 연결합니다.\n",
    "    - tf.config.experimental_connect_to_cluster(tpu)\n",
    "\n",
    "\n",
    "- initialize_tpu_system 함수를 통해 tpu시스템을 초기화합니다\n",
    "    - tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "\n",
    "- Strategy를 만들어 주고 TPUStrategy에 Resolver를 연결해서 사용 대상 TPU 집합을 데이터 분산 대상에 넣어줍니다.\n",
    "    - strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "executionInfo": {
     "elapsed": 129066,
     "status": "ok",
     "timestamp": 1602336184596,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "TWMKmCnWEwAq",
    "outputId": "cd81ea8a-2cf9-4f89-fb73-02e599dea4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "initializing  TPU ...\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.58.109.202:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.58.109.202:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU initialized\n",
      "REPLICAS: 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "DEVICE = \"TPU\"\n",
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        # print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE \n",
    "# tf.data API는 소프트웨어 파이프라이닝 방법을 tf.data.Dataset.prefetch 변환을 통해 제공합니다. 이것은 데이터가 소비되는 시간과 데이터가 생성되는 시간 간의 의존성을 줄일 수 있습니다. 특히, 이 변환은 백그라운드 스레드와 내부 버퍼를 사용하여 요청된 시간 전에 입력 데이터셋에서 요소를 가져옵니다. 가져올 요소의 수는 하나의 훈련 스텝에서 소비한 배치의 수와 같거나 커야 합니다. 이 값을 수동으로 조정하거나 tf.data.experimental.AUTOTUNE으로 설정하면 tf.data 런타임이 실행 시에 동적으로 값을 조정하도록 만듭니다.\n",
    "REPLICAS = strategy.num_replicas_in_sync #relicas의 개수를 얻습니다.\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1602460247356,
     "user": {
      "displayName": "David Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYaI7z-5kRwPA6R2AefJobuVfXBJtTIaLtVS1bHw=s64",
      "userId": "16225775759515109894"
     },
     "user_tz": -540
    },
    "id": "nnUWGqbsEs2o"
   },
   "outputs": [],
   "source": [
    "# tfrecord가 저장된 GCP PATH 설정\n",
    "# model save PATH설정\n",
    "base_path = 'gs://chkim_tfrecord/tfrecord_3천만_300'\n",
    "ROOT_PATH = f'gdrive/My Drive/Colab Notebooks/3_1천만개_batch64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju-f-13i3VGC"
   },
   "outputs": [],
   "source": [
    "os.mkdir(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ00gXHIRdLW"
   },
   "source": [
    "### 각종 매개 변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkh01_mYXk6X"
   },
   "outputs": [],
   "source": [
    "units = 1024 #디코더 gru층의 은닉상태의 크기, 어텐션 W1,W2층의 뉴런개수\n",
    "\n",
    "top_k = 36   #smiles에 등장하는 word(1글자)집합의 개수\n",
    "\n",
    "embedding_dim = 512 #word(1글자)를 512차원 vector로 임베딩하기 위함, 인코더 최종 출력도 Dense(embedding_dim)를 거침\n",
    "\n",
    "vocab_size=top_k+1 # smiles에 등장하는 word(1글자)집합의 개수(패딩까지 포함)\n",
    "\n",
    "ef = 0 # 에피션트넷B0를 사용\n",
    "\n",
    "dim = 224 #이미지 사이즈 224*224\n",
    "\n",
    "batch_size = 64\n",
    "# seq_len = 72\n",
    "\n",
    "BATCH_SIZE=64; REPLICAS=8 # 8개의 tpu core에 64개씩 배치\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "EPOCHS = 50 #학습시킬 epoch 수\n",
    "\n",
    "num_example = 10000128 #학습 이미지 개수\n",
    "\n",
    "feature_shape = 1280 \n",
    "\n",
    "attention_features_shape = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scx69A4IS6uw"
   },
   "source": [
    "## 저장된 Tokenizer를 불러와서 숫자, 문자 매핑 딕셔너리생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt38jIT3HpHA"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/gdrive/My Drive/tokenizer_word36.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7g-rzc7H9ww"
   },
   "outputs": [],
   "source": [
    "tar_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJN13PayrmvR"
   },
   "source": [
    "## 전이학습 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 분자구조이미지로부터 예측된 smiles를 출력하기 위해 \n",
    "Encoder-to-Decoder 구조에 Attention Mechanism(어텐션 메커니즘)을 추가한 모델을 사용하였습니다.\n",
    "(코드설명 마크다운 추가"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구조는 다음과 같습니다. \n",
    "\n",
    "file:///home/chkim/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C/image.png![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더 \n",
    "Encoder는 분자구조이미지텐서를 받아서 features텐서를 리턴합니다.\n",
    "\n",
    "인코더 내부의 자세한 과정에 대한 설명을 덧붙이자면 다음과 같습니다\n",
    "\n",
    "#### - 입력 : (512,224,224,3)의 분자구조 이미지 텐서 (batch크기, width, height, depth)\n",
    "    1.  인자로 받은 image tensor를 efficientnetB0을 이용하여 피쳐맵(batch크기,7,7,1280)을 추출합니다(image tensor는 tfrecord로부터 가져온 image_raw를    decode_image함수에 인자로 넣어서 얻은 값입니다.)\n",
    "    2.  dropout을 적용합니다.\n",
    "    3.  추출된 피쳐맵을 (batch크기, 49,1280)로 reshape합니다\n",
    "    4.  embedding_dim(512)크기의 Dense층을 적용하고 relu 활성화 함수를 적용합니다. \n",
    "#### - 출력 : (512, 49, 512)의 encoder 피쳐 (batch크기, 49, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gItHqoyN-5o8"
   },
   "outputs": [],
   "source": [
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6]\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model): # tf.keras.Model을 상속받음.\n",
    "  # you should define your layers in __init__ \n",
    "  def __init__(self, embedding_dim):\n",
    "      super(CNN_Encoder, self).__init__() # super : 자식 클래스에서 부모클래스의 내용을 사용하고 싶을 경우 사용.\n",
    "      self.base = EFNS[ef](input_shape=(224,224,3),weights='imagenet',include_top=False) # --> (None,7,7,1280)\n",
    "      for i, layer in enumerate(self.base.layers): \n",
    "        layer.trainable = False\n",
    "      self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "      self.fc = tf.keras.layers.Dense(embedding_dim, dtype='float32')\n",
    "\n",
    "  #you should implement the model's forward pass in call.\n",
    "  def call(self, x):\n",
    "      x = self.base(x)\n",
    "      x = self.dropout(x)\n",
    "      x = tf.reshape(x, [tf.shape(x)[0],tf.shape(x)[1]*tf.shape(x)[2],feature_shape]) # (BATCH_SIZE, 49,1280)\n",
    "      x = self.fc(x)\n",
    "      x = tf.nn.relu(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LTomG9L8Iha"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units) #units = 1024\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1) #Returns a tensor with a length 1 axis inserted at index axis.\n",
    "        \n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더\n",
    "Decoder는 RNN구조이며 바다니우 어텐션을 적용하였습니다  \n",
    "\n",
    "디코더에는 3가지 입력이 들어갑니다. \n",
    "     1.  현재 시점(t)의 word (초기 입력으로 smiles의 시작을 의미하는 '<'가 들어갑니다) tensor의 shape은 (512,49,512) 입니다\n",
    "     2.  인코더에서 나온 피쳐 \n",
    "     3.  디코더의 이전 시점(t-1) 은닉상태\n",
    "\n",
    "출력은 3가지 입니다. \n",
    "     1.  현 시점(t)의 예측 word 텐서 (batch크기,37)\n",
    "     2.  디코더의 현 시점(t) 은닉상태\n",
    "     3.  attention weight(어텐션 가중치) --> 리턴(출력)하지만 이후 사용되지는 않습니다.\n",
    "\n",
    "어텐션 메커니즘은 기존 인코더 투 디코더 모델의 컨텍스트 벡터를 개선함으로써 성능을 향상시키는 기법입니다.\n",
    "\n",
    "보통 인코더가 RNN모델에 기반해 있는 인코더 투 디코더 모델(seq2seq)은 인코더의 마지막 은닉 상태를 컨텍스트 벡터로 하고 이를 디코더 RNN 셀의 첫번재 은닉 상태로 사용합니다.  \n",
    "이 모델에서는 인코더가 CNN모델에 기반 해 있으므로 인코더의 최종 출력이 컨텍스트 벡터의 역할을 할 수 있습니다.\n",
    "\n",
    "컨텍스트 벡터를 디코더의 초기 은닉 상태로만 사용하는것에서 더 나아가 컨텍스트 벡터를 디코더가 word를 예측하는 매 시점마다 하나의 입력으로 사용할 수도 있습니다. 이에 더 나아가 어텐션 메커니즘을 사용하면 더욱 문맥을 반영할 수 있는 입력을 디코더에 줄 수 있습니다. \n",
    "\n",
    "디코더에 적용된 어텐션 메커니즘은 출력 단어를 예측하는 매 시점마다 분자이미지전체(인코더를 통해 얻은 피쳐)를 참고하는 것입니다.(기계번역 모델이라면 전체 입력 문장을 참고한다고 보면 됩니다).단 해당 시점에서 예측해야할 word와 연관이 있는 피쳐부분을 좀더 집중(attention)하여 보는 것입니다.\n",
    "\n",
    "디코더 내부의 gru층에는 \"현재 시점(t)의 word를 embedding한 vector\"와 \"context_vector\"를 연결하여 입력벡터로 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 내부의 과정에 대하여 자세하게 설명드리겠습니다.\n",
    "\n",
    "     1.  features(인코더에서 나온 피쳐), hidden(디코더의 이전 시점(t-1) 은닉상태)를 이용하여 context_vector와 attention_weight를 구합니다.\n",
    "     \n",
    "        -  feature와 hidden을 이용하여 어텐션 스코어를 구하고 이에 소프트맥스를 적용하여 attention_weight를 구하는 것입니다.\n",
    "        - 어텐션 스코어를 구하는 방법은 여러가지가 있습니다. 그중 바다니우가 제시한 'concat'방법을 사용합니다.\n",
    "        - attention_weight와 hidden을 가중합하여 context_vector를 구합니다. 이는 Attention value라고도 합니다. \n",
    "\n",
    "     2.  x(현재 시점(t)의 word)를 512차원의 vector로 변환합니다. \n",
    "\n",
    "     3.  context_vector와 x(임베딩된 word 벡터)를 연결(concatenate)하여 gru층의 입력으로 사용합니다.\n",
    "\n",
    "     4.  dropout을 거치고, 2개의 Dense layer를 거쳐 다음에 올 단어 예측값을 출력합니다\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_JlELHOuqfj"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #(37,512)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True, #메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴하고자 합니다\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.dropout =tf.keras.layers.Dropout(0.25)\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size, dtype='float32')\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        x = self.fc1(output)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strategy.scope()내에서 모델, Metric, 옵티마이저를 선언해 주어야 합니다. 그렇지 않으면 정상적으로 실행되지 않습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 139426,
     "status": "ok",
     "timestamp": 1602336209318,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "pR7JT27Kvdp6",
    "outputId": "8763d37d-3081-4cf5-c051-8492c30b10bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b1_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "27164672/27164032 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "  encoder = CNN_Encoder(embedding_dim)\n",
    "  decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "  valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "  train_loss = tf.keras.metrics.Sum() # tf.keras.metrics.Sum은 주어진 값의 합을 계산합니다.\n",
    "  valid_loss = tf.keras.metrics.Sum()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=0.00002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function\n",
    "레이블 class가 두개 이상일때 손실함수로 crossentropy loss function를 사용합니다. 이는 label(real)과 prediction(pred)간의 crossentropy loss를 계산합니다.\n",
    "\n",
    "SparseCategoricalCrossentropy함수는 레이블은 정수로 입력받으며, 원핫 벡터로 입력받고 싶으면 CategoricalCrossentropy를 사용해야합니다.  \n",
    "\n",
    "여기서 real값은 디코더에서 예측할 현 시점(t)의 입력 word(글자)입니다.즉 0부터 36사이 정수(word를 숫자로 표현한것)이며,\n",
    "pred값은 디코더에서 예측한 현 시점(t)의 예측된 37차원 word 텐서입니다 shape :(batch크기,37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred과 real값의 예시입니다.\n",
    "\n",
    "pred는 37차원 텐서 : [ 0.08155002, -0.01409034, -0.16667187, ...,  0.1283614 ,\n",
    "         0.01665916,  0.1495304 ]\n",
    "         \n",
    "real은 1차원 텐서 : 1 \n",
    "\n",
    "한번에 배치사이즈 512만큼 연산하므로 pred의 shape은 (512,37)이 되고 real의 shape은 (512,)이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ6LOywTrjod"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) #0(패딩)이 아닌것만 True로 mask한다. \n",
    "    loss_ = loss_object(real, pred) # real과  pred간의 loss를 구한다.\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype) # True 를 1로 변경한다.\n",
    "    loss_ *= mask# loss와 mask를 곱한다.\n",
    "\n",
    "    return tf.reduce_mean(loss_) #loss의 평균을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgiViChqrzaB"
   },
   "source": [
    "## GCS 에 있는 tfrecords 불러오기\n",
    "TPU는 GCS (Google Cloud Storage)에서만 학습 데이터를 읽습니다. 그 이유는 TPU의 속도가 매우 빨라서  학습시 데이터 병목 현상이 발생하기 때문입니다. TPU는 데이터를 기다리느라 대부분 유휴상태있게 되는 것입니다. \n",
    "그렇기 때문에 GCS를 이용합니다. GCS에서 여러 파일을 병렬로 스트리밍하게 되면 상당 높은 처리량을 유지할 수 있습니다.\n",
    "GCS에 데이터를 적절한 개수( 10개~ 100개) 적절하게 큰 용량( 10~100MB)로 설정하면 처리량이 최대화됩니다.\n",
    "\n",
    "tfrecord파일 하나당  256개의 이미지와 라벨을 저장했지만 512*14개(90MB)의 이미지와 smiles를 저장하고 GCS에 업로드 하는것이 좋습니다\n",
    "\n",
    "GCS 버킷을 생성하고 권한추가 --> 새 구성원:allUsers --> 역할선택 : cloud sorage,저장소 개체 뷰어 과정을 거치면 tpu에서 데이터 접근이 가능해 집니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSSoH8GQueI5"
   },
   "outputs": [],
   "source": [
    "# #gcp접근\n",
    "# from google.colab import auth, drive\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGxkMGBJHBUy"
   },
   "outputs": [],
   "source": [
    "# train_tf_records = os.path.join(base_path, '(299size400)*')\n",
    "base_path = 'gs://chkim_tfrecord/tfrecord_3천만_300' #256 * 30000 \n",
    "base_path2 = 'gs://chkim_tfrecord/tfrecord_3천만_300'# 256 * 9070\n",
    "train_tf_records = os.path.join(base_path, '*')\n",
    "train_tf_records_2 = os.path.join(base_path2, '*')\n",
    "\n",
    "train_data = np.sort(np.array(tf.io.gfile.glob(train_tf_records) + tf.io.gfile.glob(train_tf_records_2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 137769,
     "status": "ok",
     "timestamp": 1602336212515,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "coc0_xfu3uTN",
    "outputId": "08716283-e364-4951-da75-64bc633d5c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://chkim_tfrecord/tfrecord_3천만_300/batch_1000192'\n",
      " 'gs://chkim_tfrecord/tfrecord_3천만_300/batch_1000448'\n",
      " 'gs://chkim_tfrecord/tfrecord_3천만_300/batch_1000704' ...\n",
      " 'gs://chkim_tfrecord/tfrecord_3천만_300/batch_9999360'\n",
      " 'gs://chkim_tfrecord/tfrecord_3천만_300/batch_9999616'\n",
      " 'gs://chkim_tfrecord/tfrecord_3천만_300/batch_9999872']\n",
      "39070\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uSzcqQYr2gF"
   },
   "source": [
    "## 훈련데이터와 검증 데이터셋을 분리합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiIIJhr7IlgU"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=1000, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (idx, vdx) in enumerate(kfold.split(train_data)):\n",
    "    if i == 0:\n",
    "      tr_data = train_data[idx]\n",
    "      val_data = train_data[vdx]\n",
    "      \n",
    "    else:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7MaWixgWjG5"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb67GE68WXh9"
   },
   "outputs": [],
   "source": [
    "tr_data = shuffle(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 133937,
     "status": "ok",
     "timestamp": 1602336212573,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "9t419to8I0gP",
    "outputId": "97a53b04-7911-4b1f-c1b8-bb7f0a13c4e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39030"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmtfvNb9r9sf"
   },
   "source": [
    "## tfrecords를 불러오는 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 TFrecord 파일들의 경로를 GCS에서 불러와 리스트로 저장 한 후 tr_data(훈련용), val_data(검증용)으로 나눴습니다.이 둘을 아래 직접 정의한 load_dataset 함수를 통하여 모델에 feeding가능한 형태로 만들었습니다. \n",
    "\n",
    "load_dataset함수의 내부의 과정에 대해 차례대로 설명드리겠습니다.\n",
    "\n",
    "    1. 주어진 filesnames(파일경로리스트)를 가지고 tf.data.TFRecordDataset을 통해서 dataset을 생성합니다.\n",
    "    \n",
    "    2. cache()메서드를 이용하여 첫 반복에 데이터를 캐시하도록 합니다. 후속반복에 캐시된 데이터를 사용합니다\n",
    "    \n",
    "    3. repeat,과 shuffle여부를 결정합니다. shuffle시 experimental_deterministic = False로 둡니다 이는 굳이 데이터가 저장되어있는 순서대로 읽지 않음으로서 속도를 조금 빠르게 하는 것입니다.\n",
    "    \n",
    "    4. map을 이용하여 후처리작업을 수행합니다. map을 위한함수인 read_train_tfrecord는 tfrecord의 바이너리 스트림을 파싱하여 텐서로 바꿔주는 작업을 수행합니다.\n",
    "    \n",
    "    5. dataset 에 batch와 prefetch 를 적용합니다. pretetch는 데이터 입력 파이프라인 성능 최적화를 위함입니다. tf.data.experimental.AUTOTUNE으로 설정하여 tf.data런타임이 실행시에 동적으로 값을 조정하도록 만듭니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBaLzSlH1xFE"
   },
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, [224,224])\n",
    "    return image\n",
    "\n",
    "def read_train_tfrecord(example):\n",
    "    features = {\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.string), #고정길이 입력 피쳐를 parsing하기 위한 함수입니다\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, features) #serialized된 피쳐를 위에서 정의한 features형태로 파싱합니다\n",
    "    \n",
    "    data = decode_image(example['image_raw'])\n",
    "    label=example['label']\n",
    "    label = tf.io.decode_raw(label, out_type=tf.int32)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "def load_dataset(filenames, shuffle=False, repeat=False, aug=False):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.cache() #데이터 셋이 처음 반복될때 메모리에 캐시됩니다. 이후반복에는 캐시된 데이터를 사용합니다.\n",
    "    \n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(BATCH_SIZE*2)\n",
    "        opt = tf.data.Options()\n",
    "        \n",
    "        opt.experimental_deterministic = False\n",
    "        \n",
    "    dataset = dataset.map(read_train_tfrecord, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE* REPLICAS) #elements를 특정 크기로 일괄 처리하도록 결합합니다.\n",
    "    dataset = dataset.prefetch(AUTO) #현재 element가 처리 되는 동안 이후 element를 준비합니다 추가 메모리를 사용하는 대신 대기 시간이 줄어들고 처리량이 향상됩니다.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(tr_data, shuffle=True, repeat=True)\n",
    "val_dataset = load_dataset(val_data, shuffle=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental_distribute_dataset를 사용하여 데이터셋을 학습할 머신에 맞게  분배합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTUrLOVAyCpd"
   },
   "outputs": [],
   "source": [
    "# PerReplica\n",
    "train_dist_ds = strategy.experimental_distribute_dataset(train_dataset)\n",
    "valid_dist_ds = strategy.experimental_distribute_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 129606,
     "status": "ok",
     "timestamp": 1602336213038,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "kpcFZO6JDEjI",
    "outputId": "a745d768-7f1e-4a18-8b37-5633b149e55d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000128"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTa9VgXTJa6i"
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = int(num_example * 0.999) // (BATCH_SIZE * 8)\n",
    "VAL_STEPS_PER_EPOCH = int(num_example * 0.001) // (BATCH_SIZE * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련스텝정의\n",
    "train_step_fn는 위에서 생성한 train_dists_ds를 이용하여 학습을 진행하기 위한 함수입니다. \n",
    "함수 내부 실행과정에 대해 차례대로 설명을 드리겠습니다. \n",
    "    1. loss를 0으로 초기화하고 hidden(디코더의 은닉상태)을 초기화합니다. dec_input은 초기 입력으로 smiles의 시작을 의미하는 '<'가 들어갑니다\n",
    "    hidden은 shape이 (배치크기, units)인 텐서입니다. dec_input은 (배치크기,1)입니다. \n",
    "    \n",
    "    2. GradientTape내에서 이미지 텐서를 인코더에 입력해 features를 얻고 디코더에 dec_input, features hidden을 입력해 예측값을 얻습니다. 디코더의 예측은 모든 시점에 대해 반복됩니다. 시점은 72번있으며 smiles의 길이(패딩포함)입니다. 그후 pred값과 target값 사이의 loss값을 계산합니다 이후 시점 디코더의  input은 teacher forcing을 적용하기 때문에 예측값이 아니라 실제값이 들어갑니다\n",
    "    \n",
    "    3. encoder와 decoder의 훈련가능한 변수들(가중치와, 편향)을 합친 후 tape.gradient를 이용하여 미분을 구합니다.\n",
    "    \n",
    "    4. adam optimizer apply_gradients()함수로 계산된 gradient를 적용합니다.\n",
    "    \n",
    "    5. loss를 update함니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AZYgtilvibR"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_fn(img_tensor, target, validation=False):\n",
    "    loss = 0\n",
    "    \n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tar_to_index['<']] * target.shape[0], 1)\n",
    "    \n",
    "    # GradientTape안에서 계산을 하면 tape안에 계산 과정을 기록해두었다가 tape.gradient를 이용하여 미분을 자동으로 구할 수 있습니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden, training=True)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables)) #zip은 동일한 개수로 이루어진 자료형을 묶어주는 역할을 한다.\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    train_loss.update_state(total_loss)\n",
    "    #return loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증스텝 정의\n",
    "valid_step_fn은 검증 loss를 구하기 위한 함수입니다.\n",
    "\n",
    "예측값을 얻고 loss를 구하는 과정은 train_step_fn과 동일하나. gradient를 구하는 과정은 필요가 없으니 생략되었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGhsjFa6ONuq"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step_fn(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tar_to_index['<']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden, training=False)\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions) \n",
    "\n",
    "        dec_input = tf.expand_dims(target[:, i], 1) #teacher forcing\n",
    "\n",
    "    val_loss = (loss / int(target.shape[1]))\n",
    "    valid_loss.update_state(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 124302,
     "status": "ok",
     "timestamp": 1602336213632,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "qcydzmqzax-t",
    "outputId": "8da89b6b-af6d-4be1-9e7f-7da7761d7dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 가중치 불러오기\n",
    "이전에 epoch까지 학습시킨 모델이 있다면 가중치를 불러옵니다.\n",
    "가중치를 불러오기 위해서는 모델을  building하는 작업이 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPwsyEuswq28"
   },
   "source": [
    "### load_weight를 실행하기 위해 훈련 스텝을 한번 거쳐 모델을 building합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuQi8Ni4pg_Z"
   },
   "outputs": [],
   "source": [
    "train_loss.reset_states()\n",
    "valid_loss.reset_states()\n",
    "total_loss = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(train_dist_ds):\n",
    "  strategy.run(train_step_fn, args=(img_tensor, target))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D_ESXW5phft"
   },
   "source": [
    "### 이전까지 학습시킨 모델가중치를 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBt51tOo3saj"
   },
   "outputs": [],
   "source": [
    "encoder.load_weights(os.path.join(ROOT_PATH, \"eff0_fold0_molecule_encoder_epoch_1000만29_0.1078589589972245.h5\"))\n",
    "decoder.load_weights(os.path.join(ROOT_PATH, \"eff0_fold0_molecule_decoder_epoch_1000만29_0.1078589589972245.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "executionInfo": {
     "elapsed": 116932,
     "status": "ok",
     "timestamp": 1602336283024,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "wyoobJ9p6wEg",
    "outputId": "5fed192d-3acc-4ef0-d835-59abf92b3c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn__encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b1 (Model)      (None, 7, 7, 1280)        6575232   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  655872    \n",
      "=================================================================\n",
      "Total params: 7,231,104\n",
      "Trainable params: 655,872\n",
      "Non-trainable params: 6,575,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 116087,
     "status": "ok",
     "timestamp": 1602336283025,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "wIci8x3M676X",
    "outputId": "27d70a97-8cd5-4510-f4ae-cd7d901b6e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn__decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  18944     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  6297600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  37925     \n",
      "_________________________________________________________________\n",
      "bahdanau_attention (Bahdanau multiple                  1575937   \n",
      "=================================================================\n",
      "Total params: 8,980,006\n",
      "Trainable params: 8,980,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HILDY9JdUb0P"
   },
   "source": [
    "### 이전 epoch까지 decay된 learning rate 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1602336493877,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "s90LbQH_34cg",
    "outputId": "4b9dcdae-ffa0-435d-bb05-de9b827b849a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TPUMirroredVariable:{\n",
       "  0: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  1: <tf.Variable 'Adam/learning_rate/replica_1:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  2: <tf.Variable 'Adam/learning_rate/replica_2:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  3: <tf.Variable 'Adam/learning_rate/replica_3:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  4: <tf.Variable 'Adam/learning_rate/replica_4:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  5: <tf.Variable 'Adam/learning_rate/replica_5:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  6: <tf.Variable 'Adam/learning_rate/replica_6:0' shape=() dtype=float32, numpy=2e-05>,\n",
       "  7: <tf.Variable 'Adam/learning_rate/replica_7:0' shape=() dtype=float32, numpy=2e-05>\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(start_epoch) : \n",
    "  if (epoch+1)%2 == 0:\n",
    "    if optimizer.lr > 0.0001:\n",
    "      \n",
    "      optimizer.lr =  optimizer.lr * 0.69\n",
    "optimizer.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKhJhhGMsHXI"
   },
   "source": [
    "## 훈련 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 tf.keras.metrics.Sum()로 정의된 train_loss와 valid_loss를 reset_states()함수를 호출하여 metric 상태변수를 초기화합니다. \n",
    "\n",
    "train_loss는 설정한 100개의 batch마다 reset_states()를 호출함으로써 초기화합니다. valid_loss도 100개의 batch마다 val_loss를 계산하고 출력한 후 초기화됩니다. \n",
    "\n",
    "train_dist_ds에서 batch단위로 img_tensor와 target을 불러옵니다. 반복문 안에서 strategy.run()을 이용하여 train_step_fn 함수를 replica당 하나씩 호출합니다. 이렇게 하면 8개의 TPU core에서 각각 학습이 진행되고 train_loss가 update됩니다. \n",
    "\n",
    "validation도 100개의 batch마다 진행되는데 검증 후 그 시점의 val_loss 가 val_best_loss보다 낮으면 인코더와 디코더의 가중치를 저장합니다. 한 epoch이 지난 후에도 weights을 따로 저장 해 둡니다.\n",
    "\n",
    "epoch진행될수록 적절하게 learning_rate를 감소시켜야 학습이 잘 이루어 지므로 초기 lr=0.00051에서  2epoch마다. 0.69씩 곱하여 감소시킵니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "executionInfo": {
     "elapsed": 135375,
     "status": "error",
     "timestamp": 1602336488026,
     "user": {
      "displayName": "김창헌",
      "photoUrl": "",
      "userId": "05733824982842229263"
     },
     "user_tz": -540
    },
    "id": "hPMjV1pQvmXL",
    "outputId": "52feb15e-5b79-4fc4-b54a-c3f19a707718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 0 Loss 0.0010\n",
      "Epoch 30 VAL_Batch 19 Val_Loss 0.1079\n",
      "weight saved\n",
      "Epoch 30 Batch 100 Loss 0.0972\n",
      "Epoch 30 VAL_Batch 19 Val_Loss 0.1080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5549eb31f642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mglobal_has_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_as_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_get_next_as_optional\u001b[0;34m(iterator, strategy, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       worker_has_value, next_element = (\n\u001b[0;32m--> 192\u001b[0;31m           iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0;31m# Collective all-reduce requires explicit devices for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next_as_list\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1148\u001b[0m               \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_dummy_tensor_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m               \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m           )\n\u001b[1;32m   1152\u001b[0m           \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1202\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cond\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m   \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss.reset_states()\n",
    "valid_loss.reset_states()\n",
    "import time\n",
    "val_best_loss = 999\n",
    "\n",
    "for epoch in range(start_epoch, 100):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dist_ds):\n",
    "        strategy.run(train_step_fn, args=(img_tensor, target))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result().numpy() / 100/8))\n",
    "            train_loss.reset_states()\n",
    "\n",
    "\n",
    "            for val_batch, (val_img_tensor, val_target) in enumerate(valid_dist_ds):\n",
    "                strategy.run(valid_step_fn, args=(val_img_tensor, val_target))\n",
    "                if val_batch >= VAL_STEPS_PER_EPOCH:\n",
    "                  val_loss = valid_loss.result().numpy() / val_batch / 8\n",
    "                  print('Epoch {} VAL_Batch {} Val_Loss {:.4f}'.format(epoch + 1, val_batch, val_loss))\n",
    "                  if val_loss <= val_best_loss:\n",
    "                    val_best_loss = val_loss\n",
    "                    # encoder.save_weights(os.path.join(ROOT_PATH, \"eff0_fold0_molecule_encoder_1000만.h5\"))\n",
    "                    # decoder.save_weights(os.path.join(ROOT_PATH, 'eff0_fold0_molecule_decoder_1000만.h5'))\n",
    "                    print(\"weight saved\")\n",
    "                  valid_loss.reset_states()\n",
    "                  break \n",
    "        if batch > STEPS_PER_EPOCH:\n",
    "            break\n",
    "    \n",
    "    encoder.save_weights(os.path.join(ROOT_PATH, \"eff0_fold0_molecule_encoder_epoch_1000만\"+str(epoch+1)+\"_\"+str(val_best_loss)+\".h5\"))\n",
    "    decoder.save_weights(os.path.join(ROOT_PATH, 'eff0_fold0_molecule_decoder_epoch_1000만'+str(epoch+1)+\"_\"+str(val_best_loss)+\".h5\"))  \n",
    "    if (epoch+1)%2 == 0:\n",
    "      if optimizer.lr > 0.0001:\n",
    "        optimizer.lr =  optimizer.lr * 0.69\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rAOB6_9Y7tm"
   },
   "source": [
    "### 1천만개 이미지를 TPU를 이용하여 batchsize(64) * Replica(8) 로 분산학습 \n",
    "\n",
    "1Epoch당 약5700초 소요됩니다. 첫 epoch는 약 11000초 소요되는데 tpu사용시 추가적인 데이터 전처리와 캐시화가 필요하기 때문으로 보입니다. tfrecord_size를 개당 (10M~100M) 병목현상이 최소화되어 빠른 더 빠른 학습 이 가능합니다.\n",
    "학습과정중 출력된 결과는 training_output.txt에 저장되어있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE3cKsI1SX3c"
   },
   "source": [
    "### 저장된 모델파일명에 기록한 val_loss 값을 이용하여 loss_plot그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 1717,
     "status": "ok",
     "timestamp": 1602460267236,
     "user": {
      "displayName": "David Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYaI7z-5kRwPA6R2AefJobuVfXBJtTIaLtVS1bHw=s64",
      "userId": "16225775759515109894"
     },
     "user_tz": -540
    },
    "id": "Enh0KOeHFynG",
    "outputId": "c68f4265-0b95-4345-9051-809371a468a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0.242984696438438']\n",
      "['2', '0.1982471691934686']\n",
      "['3', '0.1724454227246736']\n",
      "['4', '0.1625908424979762']\n",
      "['5', '0.1515100880673057']\n",
      "['6', '0.14836613755477102']\n",
      "['7', '0.14805462485865542']\n",
      "['8', '0.14805462485865542']\n",
      "['9', '0.1385467303426642']\n",
      "['10', '0.134713750136526']\n",
      "['11', '0.1291286066958779']\n",
      "['12', '0.12674011682209216']\n",
      "['12', '0.12399919409500926']\n",
      "['13', '0.12062456733302067']\n",
      "['14', '0.11944298995168585']\n",
      "['15', '0.11691743449160927']\n",
      "['16', '0.1160430908203125']\n",
      "['17', '0.11416577038012053']\n",
      "['18', '0.11393184410898309']\n",
      "['19', '0.11344484279030248']\n",
      "['20', '0.11291651976735968']\n",
      "['21', '0.11225239854109914']\n",
      "['22', '0.11177088084973787']\n",
      "['23', '0.11151387816981266']\n",
      "['24', '0.10875595243353593']\n",
      "['25', '0.1085097915247867']\n",
      "['26', '0.10830882975929662']\n",
      "['27', '0.10817271784732216']\n",
      "['28', '0.10801583842227333']\n",
      "['29', '0.1078589589972245']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "ROOT_PATH = f'gdrive/My Drive/Colab Notebooks/3_1천만개_batch64'\n",
    "models = {}\n",
    "for m in os.listdir(ROOT_PATH) :\n",
    "  if(m.find('eff0_fold0_molecule_decoder_epoch_1000만') != -1 ): \n",
    "      epoch_loss = m.split(\"1000만\")[1]\n",
    "      epoch_loss = epoch_loss.split(\".h\")[0]\n",
    "      # print(epoch_loss)\n",
    "      epoch_loss= epoch_loss.split(\"_\")\n",
    "      print(epoch_loss)\n",
    "      epoch = epoch_loss[0]\n",
    "      loss = epoch_loss[1]\n",
    "      models[epoch] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1602460269746,
     "user": {
      "displayName": "David Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYaI7z-5kRwPA6R2AefJobuVfXBJtTIaLtVS1bHw=s64",
      "userId": "16225775759515109894"
     },
     "user_tz": -540
    },
    "id": "-2rEBdFnFnYa",
    "outputId": "9051aeea-267f-41aa-cbab-5e48ae59c104"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss_plot =[]\n",
    "for i in range(1,len(models)+1) : \n",
    "  val_loss_plot.append(float(models[str(i)]) )\n",
    "\n",
    "plt.plot(val_loss_plot, label = 'val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('val_Loss Plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0cqOJGesJkX"
   },
   "source": [
    "# 검증 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coSfcR7aGvjB"
   },
   "outputs": [],
   "source": [
    "index_to_tar = {v:k for k, v in tar_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmZpISZ-5-bM"
   },
   "outputs": [],
   "source": [
    "index_to_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "max_length = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X-0jO4jsQZd"
   },
   "source": [
    "### 추론 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIo446mmNecz"
   },
   "outputs": [],
   "source": [
    "def val_predict(img_tensor, label):\n",
    "    hidden = decoder.reset_state(batch_size=img_tensor.shape[0])\n",
    "    dec_input = tf.expand_dims([tar_to_index['<']] * img_tensor.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    result = []\n",
    "    labels = []\n",
    "    \n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predictions = tf.math.argmax(predictions, axis=1).numpy()\n",
    "        result.append(predictions)\n",
    "        labels.append(label.numpy())\n",
    "        dec_input = tf.expand_dims(predictions, 1)\n",
    "\n",
    "    return np.array(result), label.numpy(), np.array(img_tensor.numpy())\n",
    "\n",
    "# 비교적 높은 확률들로 예측\n",
    "def val_predict_(img_tensor, label):\n",
    "    hidden = decoder.reset_state(batch_size=img_tensor.shape[0])\n",
    "    dec_input = tf.expand_dims([tar_to_index['<']] * img_tensor.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    result = []\n",
    "    labels = []\n",
    "\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predictions = tf.random.categorical(predictions, 1)[:, 0].numpy()\n",
    "        result.append(predictions)\n",
    "\n",
    "\n",
    "        dec_input = tf.expand_dims(predictions, 1)\n",
    "    \n",
    "    return np.array(result), label.numpy(), np.array(img_tensor.numpy())\n",
    "\n",
    "def map_func_pred(img_name):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor\n",
    "\n",
    "val_result = []\n",
    "val_labels = []\n",
    "imgs = []\n",
    "i=0\n",
    "for batch, label in tqdm(val_dataset):\n",
    "    \n",
    "    result, labels, img = val_predict(batch, label)\n",
    "\n",
    "    val_result.append(result.T)\n",
    "    val_labels.append(labels)\n",
    "    imgs.append(img)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    if i == 2: # 메모리 부족으로 2개의 batch만 검증 진행\n",
    "      break\n",
    "\n",
    "val_result = np.vstack(val_result)\n",
    "\n",
    "val_labels = np.vstack(val_labels)\n",
    "\n",
    "imgs = np.vstack(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDWEOs_Zlclr"
   },
   "outputs": [],
   "source": [
    "print(val_result.shape, val_labels.shape, imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhzmcvlVsWnz"
   },
   "source": [
    "### 추론된 분자 구조가 preds 변수에 저장됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPFmZPh9jn-V"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for rid in range(val_result.shape[0]):\n",
    "    pred = ''.join([index_to_tar[i] for i in val_result[rid]])\n",
    "    pred = pred.split('>')[0]\n",
    "    preds.append(pred)\n",
    "error_idx = []\n",
    "for i, pred in enumerate(preds):\n",
    "    m = Chem.MolFromSmiles(pred)\n",
    "    if m == None:\n",
    "        error_idx.append(i)\n",
    "error_idx = np.array(error_idx)\n",
    "error_idx_ = error_idx.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgKN6MMEsoys"
   },
   "source": [
    "### 분자 구조가 알맞을 떄까지 지속적으로 추론을 시도합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Qr0PlgLNuLi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "drop_error = []\n",
    "\n",
    "while True:\n",
    "    error_idx_dict = {}\n",
    "    for i, e in enumerate(error_idx_):\n",
    "        error_idx_dict[i] = e\n",
    "\n",
    "\n",
    "    new_imgs = imgs[error_idx_]\n",
    "    new_labels = val_labels[error_idx_]\n",
    "    orders = list(range(len(new_imgs)))\n",
    "\n",
    "    dataset_val_ = tf.data.Dataset.from_tensor_slices((new_imgs, new_labels, orders))\n",
    "    dataset_val_ = dataset_val_.cache()\n",
    "    #dataset_val_ = dataset_val_.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset_val_ = dataset_val_.batch(BATCH_SIZE*2)\n",
    "    dataset_val_ = dataset_val_.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    val_result_ = []\n",
    "\n",
    "\n",
    "    for i, (batch, label, order) in enumerate(dataset_val_):\n",
    "\n",
    "      val_pred, _, _ = val_predict_(batch, label)\n",
    "      val_result_.append(val_pred.T)\n",
    "\n",
    "\n",
    "    val_result_ = np.vstack(val_result_)\n",
    "\n",
    "\n",
    "    preds_ = []\n",
    "    for rid in range(val_result_.shape[0]):\n",
    "        pred = ''.join([index_to_tar[i] for i in val_result_[rid] if i not in [0]])\n",
    "        pred = pred.split('>')[0]\n",
    "        preds_.append(pred)\n",
    "\n",
    "    for i, pred in enumerate(preds_):\n",
    "        m = Chem.MolFromSmiles(pred)\n",
    "        if m != None:\n",
    "            preds[error_idx_dict[i]] = pred\n",
    "            drop_idx = np.where(error_idx==error_idx_dict[i])[0]\n",
    "            drop_error.append(drop_idx[0])\n",
    "    error_idx_ = np.delete(error_idx, drop_error)\n",
    "    clear_output(wait=True)\n",
    "    print(len(list(drop_error)), '/', error_idx.shape[0])\n",
    "    \n",
    "    if error_idx.shape[0]-len(list(drop_error)) < 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKND1ChTsvNs"
   },
   "source": [
    "### 검정 데이터셋에 대한 accuracy 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8jCBfw_Nyuo"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "answer = []\n",
    "for rid, pred in enumerate(preds):\n",
    "    true = ''.join([index_to_tar[i] for i in val_labels[rid] if i not in [0]])[1:-1]\n",
    "    answer.append(true)\n",
    "    if true == pred:\n",
    "        count+=1\n",
    "print('val_accuracy : ', count/val_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS-2OAfMsxom"
   },
   "source": [
    "### similarity 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxZhXCWYN1DE"
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "for i, pred in enumerate(preds):\n",
    "    m1 = Chem.MolFromSmiles(answer[i])\n",
    "    m2 = Chem.MolFromSmiles(pred)\n",
    "    \n",
    "    if m2 != None:\n",
    "        fp1 = Chem.RDKFingerprint(m1)\n",
    "        fp2 = Chem.RDKFingerprint(m2)\n",
    "\n",
    "        similarity = DataStructs.FingerprintSimilarity(fp1,fp2)\n",
    "    else:\n",
    "        similarity = 0\n",
    "    score.append(similarity)\n",
    "    \n",
    "print('val_similarity :', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7u6popj0DQp"
   },
   "outputs": [],
   "source": [
    "print('val_similarity :', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "제출용_2.모델 생성 및 학습 코드.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
